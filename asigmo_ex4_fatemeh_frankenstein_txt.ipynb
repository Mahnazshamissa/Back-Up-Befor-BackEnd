{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "asigmo_ex4_fatemeh_frankenstein.txt.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mahnazshamissa/Back-Up-Befor-BackEnd/blob/master/asigmo_ex4_fatemeh_frankenstein_txt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLvl9qZpRR1A"
      },
      "source": [
        "# Exercise 4\n",
        "\n",
        "\n",
        "Per group create a teams-meeting where one person opens the colab notebook and shares the screen (preferably a different person than yesterday)\n",
        "\n",
        "1.   Change the data source from \"frankenstein.txt\" to \"dracula.txt\", \n",
        "2.   Train the network for 10 epochs and test it on the 3 example sentences\n",
        "3.   Write down 3 more example sentences and test the model on these as well\n",
        "4.   Train the LSTM for 50 more epochs and run the completion again. What do you observe?\n",
        "4.   Add another LSTM layer to our RNN model\n",
        "5.   Train the 2-layer LSTM for 10 epochs and see how the sentence completion has changed\n",
        "6.   Change the \"dracula.txt\" to \"trump.txt\", which contains Tweets from Donald Trump from 2019 and 2020.\n",
        "7.   Train a single-layer LSTM on the Tweet's from Trump. If you run out of memory while preprocessing the data, change ```step = 3``` to ```step = 10```.\n",
        "8.   Run the completion on the six sentences from before again. What do you observe?\n",
        "\n",
        "\n",
        "\n",
        "**Question**\n",
        "The models make very few spelling errors and few grammar error, but the content is mostly nonsense. Why do you think are the error types so different?\n",
        "\n",
        "We will meet in the general channel at 12:10!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0A4xscds3KtQ",
        "outputId": "9b79ab1b-fc66-430b-8482-f925cd5e7778",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "! nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri Oct 30 23:24:53 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.32.00    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   55C    P8    10W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTLXcbOl3n2E"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "He7DevTX5gcs"
      },
      "source": [
        "# Downloading the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhAlKCjm5f0b",
        "outputId": "e2f87cc0-2b74-4c0a-9271-5cada396f95a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "filename = \"frankenstein.txt\" \n",
        "path = tf.keras.utils.get_file(\n",
        "    filename, origin=f\"https://pub.ist.ac.at/~mlechner/datasets/{filename}\"\n",
        ")\n",
        "with open(path, encoding=\"utf-8\") as f:\n",
        "    text = f.read().lower()           # Make lowercase \n",
        "text = text.replace(\"\\n\", \" \")        # Remove line-breaks & newlines\n",
        "print(\"Corpus length:\", len(text))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://pub.ist.ac.at/~mlechner/datasets/frankenstein.txt\n",
            "434176/430265 [==============================] - 1s 1us/step\n",
            "Corpus length: 420726\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtFFQXZB5pfj",
        "outputId": "b518d94a-8d7a-4e80-8d85-a6f358b657cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Print the first 100 characters of the file we downloaded\n",
        "print(\"First 100 characters: \",text[:100])\n",
        "print(\"Last 100 characters : \",text[-100:])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First 100 characters:  frankenstein, or, the modern prometheus by mary wollstonecraft (godwin) shelley  letter 1  _to mrs. \n",
            "Last 100 characters :  h lay close to the vessel.  he was soon borne away by the waves and lost in darkness and distance.  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZ6Kxxoo6Pg1"
      },
      "source": [
        "# Define input features and output labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFTI5-543yKi",
        "outputId": "d4345ce6-cb19-47e1-88ce-0e9227a4cd90",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "chars = sorted(list(set(text)))\n",
        "print(\"Total chars:\", len(chars))\n",
        "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
        "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
        "\n",
        "maxlen = 50\n",
        "step = 3\n",
        "sentences = []\n",
        "next_chars = []\n",
        "for i in range(0, len(text) - maxlen, step):\n",
        "    sentences.append(text[i : i + maxlen])\n",
        "    next_chars.append(text[i + maxlen])\n",
        "print(\"Number of sequences:\", len(sentences))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total chars: 59\n",
            "Number of sequences: 140226\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z75uMG-35Ihz",
        "outputId": "93941b3b-e5b9-4991-d8ce-af50e5fa6fd8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"First sentence:\")\n",
        "print(sentences[0])\n",
        "print(\"First label: \")\n",
        "print(next_chars[0])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First sentence:\n",
            "frankenstein, or, the modern prometheus by mary wo\n",
            "First label: \n",
            "l\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eIkuFt65aQF"
      },
      "source": [
        "# Transforming text into numerical data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJP69sDV5VGj"
      },
      "source": [
        "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
        "y = np.zeros(len(sentences), dtype=np.int32)\n",
        "for i, sentence in enumerate(sentences):\n",
        "    for t, char in enumerate(sentence):\n",
        "        x[i, t, char_indices[char]] = 1\n",
        "    y[i] = char_indices[next_chars[i]]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KbhiVJ140xE",
        "outputId": "589a829c-db3f-4e36-b7ec-d58c9731f737",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# First training samples\n",
        "print(x[0])\n",
        "print(\"label: \")\n",
        "print(y[0])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[False False False ... False False False]\n",
            " [False False False ... False False False]\n",
            " [False False False ... False False False]\n",
            " ...\n",
            " [ True False False ... False False False]\n",
            " [False False False ... False False False]\n",
            " [False False False ... False False False]]\n",
            "label: \n",
            "34\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwfhq3pnEBit"
      },
      "source": [
        "# Splitting training and validation data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mW4OyUE4DZS9",
        "outputId": "ccad4d13-bdf6-4a8a-c8fd-750c2918e630",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# We can define our training and validation dataset as simple numpy arrays\n",
        "split_index = int(0.2*x.shape[0])\n",
        "valid_x, valid_y = x[-split_index:],y[-split_index:]\n",
        "train_x, train_y = x[:-split_index],y[:-split_index]\n",
        "print(\"valid_x.shape\",valid_x.shape)\n",
        "print(\"valid_y.shape\",valid_y.shape)\n",
        "print(\"train_x.shape\",train_x.shape)\n",
        "print(\"train_y.shape\",train_y.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "valid_x.shape (28045, 50, 59)\n",
            "valid_y.shape (28045,)\n",
            "train_x.shape (112181, 50, 59)\n",
            "train_y.shape (112181,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCurnl9YOAM1"
      },
      "source": [
        "# Defining our Recurrent Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Se-1ndKD4E4i",
        "outputId": "88cba3fa-19f7-49cd-a9d8-9ab8191820ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = tf.keras.Sequential(\n",
        "    [\n",
        "        tf.keras.Input(shape=(maxlen, len(chars))),\n",
        "        tf.keras.layers.LSTM(128),\n",
        "        tf.keras.layers.Dense(len(chars), activation=\"softmax\"),\n",
        "    ]\n",
        ")\n",
        "optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.01)\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,metrics=[\"sparse_categorical_accuracy\"])\n",
        "model.summary()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm (LSTM)                  (None, 128)               96256     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 59)                7611      \n",
            "=================================================================\n",
            "Total params: 103,867\n",
            "Trainable params: 103,867\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZ0PMEJlRDyd"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1NLlj4z4bkK",
        "outputId": "e550d534-71f7-464a-cc6f-2ffe558db180",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "training_log = model.fit(train_x, train_y, batch_size=64, epochs=10, validation_data=(valid_x,valid_y))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1753/1753 [==============================] - 11s 6ms/step - loss: 1.9761 - sparse_categorical_accuracy: 0.4172 - val_loss: 1.7010 - val_sparse_categorical_accuracy: 0.4883\n",
            "Epoch 2/10\n",
            "1753/1753 [==============================] - 10s 6ms/step - loss: 1.6157 - sparse_categorical_accuracy: 0.5174 - val_loss: 1.6018 - val_sparse_categorical_accuracy: 0.5248\n",
            "Epoch 3/10\n",
            "1753/1753 [==============================] - 10s 6ms/step - loss: 1.5089 - sparse_categorical_accuracy: 0.5501 - val_loss: 1.5679 - val_sparse_categorical_accuracy: 0.5346\n",
            "Epoch 4/10\n",
            "1753/1753 [==============================] - 10s 6ms/step - loss: 1.4542 - sparse_categorical_accuracy: 0.5655 - val_loss: 1.5356 - val_sparse_categorical_accuracy: 0.5436\n",
            "Epoch 5/10\n",
            "1753/1753 [==============================] - 10s 6ms/step - loss: 1.4173 - sparse_categorical_accuracy: 0.5742 - val_loss: 1.5282 - val_sparse_categorical_accuracy: 0.5535\n",
            "Epoch 6/10\n",
            "1753/1753 [==============================] - 10s 6ms/step - loss: 1.3907 - sparse_categorical_accuracy: 0.5812 - val_loss: 1.5324 - val_sparse_categorical_accuracy: 0.5494\n",
            "Epoch 7/10\n",
            "1753/1753 [==============================] - 10s 6ms/step - loss: 1.3648 - sparse_categorical_accuracy: 0.5885 - val_loss: 1.5225 - val_sparse_categorical_accuracy: 0.5516\n",
            "Epoch 8/10\n",
            "1753/1753 [==============================] - 10s 6ms/step - loss: 1.3491 - sparse_categorical_accuracy: 0.5925 - val_loss: 1.5267 - val_sparse_categorical_accuracy: 0.5577\n",
            "Epoch 9/10\n",
            "1753/1753 [==============================] - 10s 6ms/step - loss: 1.3334 - sparse_categorical_accuracy: 0.5963 - val_loss: 1.5269 - val_sparse_categorical_accuracy: 0.5546\n",
            "Epoch 10/10\n",
            "1753/1753 [==============================] - 10s 6ms/step - loss: 1.3188 - sparse_categorical_accuracy: 0.5988 - val_loss: 1.5374 - val_sparse_categorical_accuracy: 0.5575\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-F2scl-RFjz"
      },
      "source": [
        "# Let's try out our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGqx_9q-A6kb"
      },
      "source": [
        "def sample(preds, temperature=0.5):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype(\"float64\")\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)\n",
        "    \n",
        "def complete_sentence(sentence):\n",
        "  if len(sentence) < maxlen:\n",
        "    raise ValueError(f\"Sentence must have at least {maxlen} characters, but provided sentence had only {sentence}\")\n",
        "  if len(sentence) > maxlen:\n",
        "    sentence = sentence[-maxlen:]\n",
        "  generated = \"\"\n",
        "  print(f\"Continuing sentence '{sentence}' ...\")\n",
        "\n",
        "  for i in range(200):\n",
        "      x_pred = np.zeros((1, maxlen, len(chars)))\n",
        "      for t, char in enumerate(sentence):\n",
        "          x_pred[0, t, char_indices[char]] = 1.0\n",
        "      preds = model.predict(x_pred, verbose=0)[0]\n",
        "      next_index = sample(preds)\n",
        "      next_char = indices_char[next_index]\n",
        "      sentence = sentence[1:] + next_char\n",
        "      generated += next_char\n",
        "\n",
        "  print(\"... \", generated)\n",
        "  print()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZAW4zpVBgzj",
        "outputId": "feb49580-e063-4a05-aa88-1360c86999a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "complete_sentence(\"this is an example to show how the machine would continue\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Continuing sentence ' an example to show how the machine would continue' ...\n",
            "...  d of happiny and all the stat of the bearnes of the strent which my strangelf to make the mists of the studinb, and i felt that i am as a commence of the strenge with you could descended to the moon, \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQBfHwCJUmh2",
        "outputId": "35b61c0c-e305-47c8-e9ee-d0e23589de87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "### TODO: Add 3 more sentences to complete. Think about what types of sentences would be interesting to test\n",
        "\n",
        "complete_sentence(\"this is an example to show how the machine would continue\")\n",
        "complete_sentence(\"I am a power woman and mother and i was a happy girl in my life \")\n",
        "complete_sentence(\"i am a power girl because in my entire life i am happy and hopefull\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Continuing sentence ' an example to show how the machine would continue' ...\n",
            "...  d in the promise of the work for his streng with the state of my unmaterable to all the strange and shatter and that he and in the charmanch of the fiend of the cour instruction to and the strenge wit\n",
            "\n",
            "Continuing sentence 'oman and mother and i was a happy girl in my life ' ...\n",
            "...  some my whole the presence of your inevise the father, which should have so england the moon to the scenes of the countenance of the mind to his present the protents of the beauty of describe relation\n",
            "\n",
            "Continuing sentence ' because in my entire life i am happy and hopefull' ...\n",
            "...  y to her who had describe to discover his state of selfming my sound of the thinks of the companio“s, and his secalied made the strection of the fathy of the living the studinbs of the since of the mo\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtcRdawJPE9s"
      },
      "source": [
        "# Biased data -> biased model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ij4obo3uPKOq",
        "outputId": "2282002b-d16d-4c15-c076-656c94e80121",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "complete_sentence(\"    was the thiefs answer, to which the man replied \")\n",
        "complete_sentence(\"    was the thiefs answer, to which the woman replied \")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Continuing sentence '  was the thiefs answer, to which the man replied ' ...\n",
            "...  that i alone, and the scene of my food the scenes of the beauty of the lase, and the strenge more men had donmed in my their to my mind that i appeared to the labours of the seck with the proverdicant\n",
            "\n",
            "Continuing sentence 'was the thiefs answer, to which the woman replied ' ...\n",
            "...  my sky of the fullic and strowe with a become our death.  the rain of callestion of a seckinns, and at the more and unbranches.  about which i had desired to and the streins of the means of the more o\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}